---
title: "Introduction to the GLOSSA workflow in R with a toy example"
subtitle: "Version 0.1.0"
author: "Jorge Mestre TomÃ¡s"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_document:
    toc: TRUE
    toc_depth: 2
    number_sections: TRUE
vignette: >
  %\VignetteIndexEntry{Introduction to the GLOSSA workflow in R with a toy example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)
```

# Introduction

*Briefly introduce GLOSSA, BART and the simplicity of the toy example*

*Overview of GLOSSA workflow image*

# Load dependencies

```{r}
# Load libraries
library(glossa)
library(dplyr)
library(ggplot2)

# Load world map
sf::sf_use_s2(FALSE)
study_area_poly <- rnaturalearth::ne_countries(scale = "medium", returnclass = "sf") %>% 
  sf::st_as_sfc() %>%
  sf::st_union() %>%
  sf::st_make_valid() %>%
  sf::st_wrap_dateline() %>% 
  glossa::invert_polygon(bbox = c(xmin = -180, ymin =-90, xmax = 180, ymax = 90))

non_study_area_poly <- glossa::invert_polygon(study_area_poly)
```

# Load presence(/absence) data and environmental layers

In order to run GLOSSA, we need the species occurrences (and optionally, the absences) as well as the covariate layers corresponding to the time points when those presences were recorded.

In this toy example, we are going to predict the distribution of two species (`sp1` and `sp2`) based on two covariates (`x1` and `x2`) across two different future scenarios (`future1_layers.zip` and `future2_layers.zip`). Visit [How to format input data](https://github.com/jmestret/glossa) for a description of the input file formats.

The layers are passed to GLOSSA as a zipped directory, containing one subdirectory for each covariate and one raster layer for each time point. The future layers provided should have the same structure but include as many raster files as there are future time points you wish to predict:

```
historical_layers.zip           future1_layers.zip            future2_layers.zip
|__ x1                          |__ x1                        |__ x1
|   |__ x1_1.tif                |   |__ x1_3A.tif             |   |__ x1_3B.tif
|   |__ x1_2.tif                |__ x2                        |__ x2
|__ x2                              |__ x2_3A.tif                 |__ x2_3B.tif
    |__ x2_1.tif
    |__ x2_2.tif
```

```{r}
# Load presence(/absence) data
pa_file <- "../inst/extdata/sp2.csv"
raw_pa <- glossa::read_glossa_pa(pa_file)

# Load historical layers
historical_files <- "../inst/extdata/historical_layers.zip"
past_layers <- glossa::read_glossa_layers(historical_files)

# Load future layers
future_files <- "../inst/extdata/future1_layers.zip"
future_layers <- glossa::read_glossa_layers(future_files)
```

```{r, echo = FALSE}
ggplot() +
  geom_sf(data = non_study_area_poly, color = "#353839", fill = "antiquewhite") +
  geom_point(data = raw_pa, aes(x = decimalLongitude, y = decimalLatitude, color = "#F6733A")) +
  theme(
    panel.grid.major = element_line(
      color = gray(.5),
      linetype = "dashed",
      linewidth = 0.5
    ),
    panel.background = element_rect(fill = "white"),
    axis.title = element_blank(),
    legend.position = "bottom"
  )
```

# Clean coordinates

First, we will clean up some unwanted coordinates. This is a straightforward preprocessing step, as we assume the user has already provided the coordinates, whether processed or not, according to their preference. In this step, we focus on removing duplicate coordinates based on a specified coordinate resolution (use `n_digits` to select the number of decimal places in the coordinates). We also remove longitudinal duplicates (duplicated points across time) because we will be fitting the model by aggregating all observations across the years, as if we were creating a single aggregated snapshot of the historical data. Finally, we remove points that fall outside the marine regions according to the `rnaturalearth` world polygon.

```{r}
coords <- c("decimalLongitude", "decimalLatitude")

# Remove NA coordinates
clean_pa <- raw_pa[complete.cases(raw_pa[, coords]), ]

# Round coordinates to 4 decimal digits (this is up to your choice and the precision you need)
decimal_digits <- 4
clean_pa[, coords[1]] <- round(clean_pa[, coords[1]], decimal_digits)
clean_pa[, coords[2]] <- round(clean_pa[, coords[2]], decimal_digits)

# Remove duplicated points
clean_pa <- glossa::remove_duplicate_points(clean_pa, coords = coords)

# Remove points outside the ocean boundaries (overlapping land)
clean_pa <- glossa::remove_points_poly(clean_pa,
                                       sf_poly = study_area_poly,
                                       overlapping = FALSE,
                                       coords = coords)
```

You can summarize all these steps by just running the `glossa::clean_coordinates()` function:

```{r}
clean_pa <- glossa::clean_coordinates(
  data = raw_pa,
  sf_poly = study_area_poly,
  overlapping = FALSE,
  decimal_digits = decimal_digits,
  coords = coords
)
```

# Covariate layer processing

First, we will standardize the environmental layers to a global scale using a predefined ocean mask. This step involves cropping and extending the raster layers to align them properly:

```{r}
past_layers <- lapply(past_layers, function(x){
  glossa::layer_mask(layers = x, sf_poly = study_area_poly)
})

future_layers <- lapply(future_layers, function(x){
  glossa::layer_mask(layers = x, sf_poly = study_area_poly)
})
```

Before fitting the model and making predictions, we are going to standardize the variables. We achieve this by calculating the mean and standard deviation of all past variables and using these statistics to scale both past and future layers:

```{r}
# Compute mean and sd of the historical time series for each environmental variable
past_mean <- lapply(past_layers, function(x){
  mean(as.vector(x), na.rm = TRUE)
})

past_sd <- lapply(past_layers, function(x){
  sd(as.vector(x), na.rm = TRUE)
})

# Scale layers with past mean and sd
past_layers <- lapply(seq_along(past_layers), function(x, n, i){
  terra::scale(x[[n[i]]], center = past_mean[[n[i]]], scale = past_sd[[n[i]]])},
  x = past_layers,
  n = names(past_layers))
names(past_layers) <- names(past_mean)

future_layers <- lapply(seq_along(future_layers), function(x, n, i){
  terra::scale(x[[n[i]]], center = past_mean[[n[i]]], scale = past_sd[[n[i]]])},
  x = future_layers,
  n = names(future_layers))
names(future_layers) <- names(past_mean)
```

Next, we want to create a single snapshot by averaging all the scaled past layers. This combined layer will be used to fit the model:

```{r}
# Get mean of scaled past layers for model fitting
historical_layers <- lapply(past_layers, function(x){
  terra::mean(x, na.rm = TRUE)
})
historical_layers <- terra::rast(historical_layers)
```

# Build model matrix and pseudoabsences

First, we need to ensure that there are no NA values in the covariates associated with our presence/absence data:

```{r}
y_resp <- cbind(clean_pa,
                  terra::extract(historical_layers, clean_pa[, coords])) %>%
  tidyr::drop_na()  %>%
  dplyr::select(colnames(clean_pa))
```

Since only occurrences (presences) are provided, we must generate pseudoabsences (random and balanced):

```{r}
# Generate balanced random pseudoabsences
if (all(y_resp[, "pa"] == 1)){
  set.seed(1234)
  y_resp <- glossa::generate_pseudo_absences(y_resp, study_area_poly, historical_layers)
}
```

# Fit native range and suitable habitat model

To fit the native ranges we are going to add longitude and latitude as covariates to constrain the suitability in the space. For so we must create a layer with the coordinate values:

```{r}
coords_layer <- glossa::create_coords_layer(historical_layers, study_area_poly, scale_layers = TRUE)
```

Now we can fit the model for the native range and the suitable habitat

```{r}
model_native_range <- fit_bart_model(
  y_resp,
  c(historical_layers, coords_layer),
  seed = 1234
)

model_suitable_habitat <- fit_bart_model(
  y_resp,
  historical_layers,
  seed = 1234
)
```

# Predict past, historical and future

```{r}
historical_native_range <- glossa::predict_bart(model_native_range, c(historical_layers, coords_layer))
historical_suitable_habitat <- glossa::predict_bart(model_suitable_habitat, historical_layers)
```

```{r}
# Past prediction
past_suitable_habitat <- lapply(1:terra::nlyr(past_layers[[1]]), function(i){
  # Stack covariates by year
  pred_layers <- lapply(past_layers, function(y){
    return(y[[i]])
  })
  pred_layers <- terra::rast(pred_layers)
  
  glossa::predict_bart(model_suitable_habitat, pred_layers)
})

future_suitable_habitat <- lapply(1:terra::nlyr(future_layers[[1]]), function(i){
  # Stack covariates by year
  pred_layers <- lapply(future_layers, function(y){
    return(y[[i]])
  })
  pred_layers <- terra::rast(pred_layers)
  
  glossa::predict_bart(model_suitable_habitat, pred_layers)
})
```







