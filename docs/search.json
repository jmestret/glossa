[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Below is a list of research articles, reports, and publications that have utilized GLOSSA for species distribution modeling.\n\n\n\n\n\nFuster-Alonso, A., Mestre-Tomás, J., Baez, J. C., Pennino, M. G., Barber, X., Bellido, J. M., … & Coll, M. (2024). Machine learning applied to global scale species distribution models (SDMs). PREPRINT (Version 1) available at Research Square. doi: https://doi.org/10.21203/rs.3.rs-4411399/v1\n\n\n\n\n\nHave you used GLOSSA in your research? We’d love to feature your publication here! Please contact us with the details of your work."
  },
  {
    "objectID": "publications.html#full-list-of-publications",
    "href": "publications.html#full-list-of-publications",
    "title": "Publications",
    "section": "",
    "text": "Fuster-Alonso, A., Mestre-Tomás, J., Baez, J. C., Pennino, M. G., Barber, X., Bellido, J. M., … & Coll, M. (2024). Machine learning applied to global scale species distribution models (SDMs). PREPRINT (Version 1) available at Research Square. doi: https://doi.org/10.21203/rs.3.rs-4411399/v1"
  },
  {
    "objectID": "publications.html#submit-a-publication",
    "href": "publications.html#submit-a-publication",
    "title": "Publications",
    "section": "",
    "text": "Have you used GLOSSA in your research? We’d love to feature your publication here! Please contact us with the details of your work."
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html",
    "title": "Example 3",
    "section": "",
    "text": "This example shows how to use GLOSSA to predict the suitable habitat of Siganus luridus in the Greek Seas and identify potential areas at risk of invasion. We achieve this by comparing the species’ native range with its projected suitable habitat. Occurrence data (2000-2020) is obtained from the GreekMarineICAS geodataset, created under the ALAS: Aliens in the Aegean – A Sea Under Siege project (https://alas.edu.gr/). Environmental data is obtained from the EU Copernicus Marine Environment Monitoring Service (https://marine.copernicus.eu/).\nWe start by loading the required R packages.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#introduction",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#introduction",
    "title": "Example 3",
    "section": "",
    "text": "This example shows how to use GLOSSA to predict the suitable habitat of Siganus luridus in the Greek Seas and identify potential areas at risk of invasion. We achieve this by comparing the species’ native range with its projected suitable habitat. Occurrence data (2000-2020) is obtained from the GreekMarineICAS geodataset, created under the ALAS: Aliens in the Aegean – A Sea Under Siege project (https://alas.edu.gr/). Environmental data is obtained from the EU Copernicus Marine Environment Monitoring Service (https://marine.copernicus.eu/).\nWe start by loading the required R packages.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#data-preparation",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#data-preparation",
    "title": "Example 3",
    "section": "Data preparation",
    "text": "Data preparation\n\nDownload occurrence data\nWe download occurrence data for Siganus luridus from the MedOBIS node, specifically from the GreekMarineICAS geodataset, part of the ALAS project. The dataset is available here (Sini et al, 2024).\nAfter downloading and unzipping the data, we filter it to retain records of Siganus luridus from 2000 to 2020, aligning with the environmental data’s timeframe. Since all records are presences, we set the pa column to 1. We then format the data, saving it as a tab-separated file with decimalLongitude, decimalLatitude, timestamp, and pa columns.\n\n# Unzip and read the data\ntmpdir &lt;- tempdir()\nzip_contents &lt;- utils::unzip(\"data/dwca-greekmarineicas_geodataset-v2.0.zip\", \n                             unzip = getOption(\"unzip\"), exdir = tmpdir)\nluridus &lt;- list.files(tmpdir, pattern = \"\\\\.txt$\", full.names = TRUE)\nluridus &lt;- read.csv2(luridus, header = TRUE, sep = \"\\t\")\n\n# Filter data for *Siganus luridus*\nluridus &lt;- luridus[luridus$scientificName == \"Siganus luridus\", ]\n\n# Select and rename columns of interest\nluridus &lt;- luridus[, c(\"decimalLongitude\", \"decimalLatitude\", \n                       \"eventDate\", \"occurrenceStatus\")]\nluridus$eventDate &lt;- as.numeric(sapply(strsplit(luridus$eventDate, \"/|-\"), \n                                       function(x) x[[1]]))\n\n# Filter data by event date to match COPERNICUS layers (2000-2020)\nluridus &lt;- luridus[luridus$eventDate &gt;= 2000 & luridus$eventDate &lt;= 2020, ]\n\n# Convert occurrence status to binary presence/absence\ntable(luridus$occurrenceStatus)\nluridus$occurrenceStatus &lt;- 1\ncolnames(luridus) &lt;- c(\"decimalLongitude\", \"decimalLatitude\", \"timestamp\", \"pa\")\n\n# Remove incomplete occurrences\nluridus &lt;- luridus[complete.cases(luridus), ]\n\n# Save cleaned data to file\nwrite.table(luridus, file = \"data/Siganus_luridus_occ.csv\", sep = \"\\t\", \n            dec = \".\", quote = FALSE)\n\n\n\nDownload environmental data\nWe download environmental data from the Copernicus Marine Service using their Python API. The data includes sea water temperature (thetao) and salinity (so) from the Mediterranean Sea Physics Reanalysis, and primary production (nppv) and dissolved oxygen (o2) from the Mediterranean Sea Biogeochemistry Reanalysis product. The data is downloaded yearly (2000-2020) at a grid resolution of 1/24 degrees for a depth range of 2 to 40 meters, which corresponds to the species’ preferred habitat, as described in FishBase (Gothel, 1992).\nimport copernicusmarine\n\n# Download temperature data version \"202211\"\ncopernicusmarine.subset(\n  dataset_id=\"cmems_mod_med_phy-tem_my_4.2km_P1Y-m\",\n  variables=[\"thetao\"],\n  minimum_longitude=19,\n  maximum_longitude=30,\n  minimum_latitude=34,\n  maximum_latitude=41.1,\n  start_datetime=\"2000-01-01T00:00:00\",\n  end_datetime=\"2020-12-31T23:59:00\",\n  minimum_depth=2,\n  maximum_depth=40,\n)\n\n# Download salinity data version \"202211\"\ncopernicusmarine.subset(\n  dataset_id=\"cmems_mod_med_phy-sal_my_4.2km_P1Y-m\",\n  variables=[\"so\"],\n  minimum_longitude=19,\n  maximum_longitude=30,\n  minimum_latitude=34,\n  maximum_latitude=41.1,\n  start_datetime=\"2000-01-01T00:00:00\",\n  end_datetime=\"2020-12-31T23:59:00\",\n  minimum_depth=2,\n  maximum_depth=40,\n)\n\n# Download biogeochemical data version \"202211\"\ncopernicusmarine.subset(\n  dataset_id=\"cmems_mod_med_bgc-bio_my_4.2km_P1Y-m\",\n  variables=[\"nppv\", \"o2\"],\n  minimum_longitude=19,\n  maximum_longitude=30,\n  minimum_latitude=34,\n  maximum_latitude=41.1,\n  start_datetime=\"2000-01-01T00:00:00\",\n  end_datetime=\"2020-12-31T23:59:00\",\n  minimum_depth=2,\n  maximum_depth=40,\n)\nOnce the data is downloaded, we compute the mean values of the environmental variables within the specified depth range. Apart from the dynamic variables, we also include the bathymetry as a static variable. We obtained the bathymetry from the ETOPO Global Relief Model, which can be downloaded from here. We downloaded the bedrock elevation netCDF version ETOPO 2022 with a 60 arc-second resolution.\n\n# Load biogeochemical variables\nbiogeochem_variables &lt;- terra::rast(\"data/cmems_mod_med_bgc-bio_my_4.2km_P1Y-m_nppv-o2_19.00E-30.00E_34.02N-41.06N_3.17-37.85m_2000-01-01-2020-01-01.nc\")\n\n# Extract and process layer names\nlayer_names &lt;- names(biogeochem_variables)\nlayer_names &lt;- strsplit(layer_names, \"_\")\nlayer_names &lt;- do.call(rbind, layer_names)\nlayer_names &lt;- as.data.frame(layer_names)\ncolnames(layer_names) &lt;- c(\"variable\", \"depth\", \"year\")\n\n# Compute mean values for each year across depths\nenv_data_year &lt;- list(\"nppv\" = list(), \"o2\" = list(), \"thetao\" = list(), \"so\" = list()) \nfor (variable in unique(layer_names$variable)){\n  for (year in unique(layer_names$year)){\n    indices &lt;- which(layer_names$variable == variable & layer_names$year == year)\n    print(paste(variable, year))\n    mean_water_column &lt;- terra::mean(biogeochem_variables[[indices]], na.rm = TRUE)\n    env_data_year[[variable]][[year]] &lt;- mean_water_column\n  }\n}\n\n# Load physical variables\nphysical_variables &lt;- c(\n  terra::rast(\"data/cmems_mod_med_phy-tem_my_4.2km_P1Y-m_thetao_19.00E-30.00E_34.02N-41.06N_3.17-37.85m_2000-01-01-2020-01-01.nc\"),\n  terra::rast(\"data/cmems_mod_med_phy-sal_my_4.2km_P1Y-m_so_19.00E-30.00E_34.02N-41.06N_3.17-37.85m_2000-01-01-2020-01-01.nc\")\n)\n\n# Process physical variables\nlayer_names &lt;- names(physical_variables)\nlayer_names &lt;- strsplit(layer_names, \"_\")\nlayer_names &lt;- do.call(rbind, layer_names)\nlayer_names &lt;- as.data.frame(layer_names)\ncolnames(layer_names) &lt;- c(\"variable\", \"depth\", \"year\")\n\n# Mean between different depths\nfor (variable in unique(layer_names$variable)){\n  for (year in unique(layer_names$year)){\n    indices &lt;- which(layer_names$variable == variable & layer_names$year == year)\n    print(paste(variable, year))\n    mean_water_column &lt;- terra::mean(physical_variables[[indices]], na.rm = TRUE)\n    env_data_year[[variable]][[year]] &lt;- mean_water_column\n  }\n}\n\n# Load and process bathymetry data\nbat &lt;- terra::rast(\"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\nbat &lt;- -1*bat\nbat &lt;- terra::aggregate(bat, fact = 5, fun = \"mean\")\nr &lt;- terra::rast(terra::ext(env_data_year[[1]][[1]]), \n                 res = terra::res(env_data_year[[1]][[1]]))\nbat &lt;- terra::resample(bat, r)\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save processed layers to files\ndir.create(\"data/fit_layers\")\ndir.create(\"data/fit_layers/bat\")\ndir.create(\"data/fit_layers/thetao\")\ndir.create(\"data/fit_layers/so\")\ndir.create(\"data/fit_layers/nppv\")\ndir.create(\"data/fit_layers/o2\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(\n      env_data_year[[j]][[i]], \n      filename = paste0(\"data/fit_layers/\", j ,\"/\", j, \"_\", i, \".tif\")\n    )\n  }\n}\n\n# Compress the layers into a zip file\nzip(zipfile = \"data/fit_layers.zip\", files = \"data/fit_layers\")\n\nAdditionally, we created a proj_layers.zip file containing the layers for 2020, allowing us to predict the native range and suitable habitat under “present” conditions.\nproj_layers.zip\n    ├───bat\n    │       bat_21.tif\n    ├───nppv\n    │       nppv_21.tif\n    ├───o2\n    │       o2_21.tif\n    ├───so\n    │       so_21.tif\n    └───thetao\n            thetao_21.tif"
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#glossa-modeling",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#glossa-modeling",
    "title": "Example 3",
    "section": "GLOSSA modeling",
    "text": "GLOSSA modeling\nWith the data formatted and ready, we run the GLOSSA Shiny app.\n\nrun_glossa()\n\nWe upload the occurrence file for S. luridus and the environmental layers for model fitting and projection (in this case, the year 2020). To compute the native range and suitable habitat, we select the Model fitting and Model projection options for both the Native range and Suitable habitat models. Additionally, we enable Functional responses, Variable importance and Cross-validation in the Others section.\nIn the advanced options, we standardize the environmental data, choose the BART model (Chipman, et al., 2010; Dorie, 2024), and set the seed to 1984 for reproducibility.\n\n\n\nAnalysis options set in GLOSSA for modeling the distribution of Siganus luridus."
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#results",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#results",
    "title": "Example 3",
    "section": "Results",
    "text": "Results\nThe model was fitted with 256 presences and 256 pseudo-absences. Many presence points were excluded due to their proximity to the coast, where the environmental layers from COPERNICUS lack data. Future analyses could benefit from using environmental data sources with better coastal resolution or imputing values. For this example, this exclusion demonstrates how GLOSSA handles these situations.\n\n\n\nDiscarded records of Siganus luridus during occurrence processing.\n\n\nDespite the data limitations, the model summary indicates good performance, with clear classification between presences and pseudo-absences for both models.\n\n\n\nSummary of the fitted models for Siganus luridus.\n\n\nSimilarly, the 5-fold cross-validation results show a high F-score for both the native range and suitable habitat models, suggesting strong predictive performance. However, keep in mind that this k-fold cross-validation is random and does not use temporal or spatial blocks, which could be relevant for more robust assessments.\n\n\n\nK-fold cross-validation for the native range and suitable habitat models.\n\n\nThe figure below shows the native range and suitable habitat predictions for 2020. The native range represents the species’ current distribution, considering spatial smoothing with latitude and longitude included as predictors in the model. In contrast, the suitable habitat map highlights areas with favorable environmental conditions for the species, which may not yet be occupied.\n\n\n\nPredicted native range and suitable habitat of Siganus luridus in 2020.\n\n\nFor example, the Thracian Sea and northeast Aegean islands were identified as high-risk areas for potential invasion, even though the species is not currently established there according to the predicted native range in 2020.\n\n\n\nDifference between mean suitable habitat and native range."
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#conclusion",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#conclusion",
    "title": "Example 3",
    "section": "Conclusion",
    "text": "Conclusion\nUsing GLOSSA and environmental data, we successfully modeled the potential suitable habitat for Siganus luridus and identified areas at risk of invasion. This information is critical for understanding and managing the impacts of invasive species on marine ecosystems."
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#computation-time",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#computation-time",
    "title": "Example 3",
    "section": "Computation time",
    "text": "Computation time\nThe table below summarizes the computation times for the various steps of the GLOSSA analysis, providing an overview of the computational resources required at each step. The analysis was performed on a single Windows 11 machine equipped with 64 GB of RAM and an Intel(R) Core(TM) i7-1165G7 processor. This processor features 4 cores and 8 threads, with a base clock speed of 2.80 GHz.\n\nTable 1. Computation times for different steps of the GLOSSA analysis for Siganus luridus in the Greek Seas.\n\n\n\n\nTask\nExecution Time\n\n\n\n\nLoading input data\n1.92 secs\n\n\nProcessing P/A coordinates\n0.004 secs\n\n\nProcessing covariate layers\n1.23 secs\n\n\nBuilding model matrix\n1.26 secs\n\n\nFitting native range models\n0.018 mins\n\n\nVariable importance (native range)\n0.60 mins\n\n\nP/A cutoff (native range)\n0.009 mins\n\n\nProjections on fit layers (native range)\n0.997 mins\n\n\nNative range projections\n0.42 mins\n\n\nNative range\n1.45 mins\n\n\nFitting suitable habitat models\n0.016 mins\n\n\nVariable importance (suitable habitat)\n0.41 mins\n\n\nP/A cutoff (suitable habitat)\n0.008 mins\n\n\nProjections on fit layers (suitable habitat)\n0.79 mins\n\n\nSuitable habitat projections\n0.38 mins\n\n\nHabitat suitability\n0.002 mins\n\n\nSuitable habitat\n1.20 mins\n\n\nComputing functional responses\n0.69 mins\n\n\nCross-validation\n0.48 mins\n\n\nModel summary\n0.016 mins\n\n\nTotal GLOSSA analysis\n3.91 mins"
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#references",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#references",
    "title": "Example 3",
    "section": "References",
    "text": "References\n\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees.\nDorie V (2024). dbarts: Discrete Bayesian Additive Regression Trees Sampler. R package version 0.9-28, https://CRAN.R-project.org/package=dbarts.\nGothel, H. (1992). Fauna marina del Mediterráneo. Ediciones Omega, SA, Barcelona, 319.\nSini M, Ragkousis M, Koukourouvli N, Katsanevakis S & Zenetos A (2024). Marine impactful cryptogenic and alien species in the Greek Seas: A georeferenced dataset (1893-2020). Version 2.0. Hellenic Center for Marine Research. Occurrence dataset. https://doi.org/10.25607/t2smha"
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html",
    "title": "Example 2",
    "section": "",
    "text": "This vignette provides a detailed example of fitting a single-species distribution model within a user-defined region. We will walk through the steps of obtaining occurrence data, gathering environmental data, and defining the study area using a polygon. The example focuses on the distribution of the loggerhead sea turtle (Caretta caretta) in the Mediterranean Sea. Occurrence data will be retrieved from GBIF for the years 2000 to 2019, and environmental data from Bio-ORACLE v3.0 (Assis et al., 2024). We will use GLOSSA to fit a species distribution model and project it under different future climate scenarios.\nTo get started, we load the glossa package, as well as terra (Hijmans, 2024) and sf (Pebesma, 2018) to work with spatial rasters and vector data. We also load rgbif (Chamberlain, 2017) and biooracler (Fernandez Bejarano and Salazar, 2024) for downloading species occurrences and environmental data, respectively. Additionally, the dplyr (Wickham et al., 2023) package will be used for data manipulation.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(rgbif)\nlibrary(biooracler)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#introduction",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#introduction",
    "title": "Example 2",
    "section": "",
    "text": "This vignette provides a detailed example of fitting a single-species distribution model within a user-defined region. We will walk through the steps of obtaining occurrence data, gathering environmental data, and defining the study area using a polygon. The example focuses on the distribution of the loggerhead sea turtle (Caretta caretta) in the Mediterranean Sea. Occurrence data will be retrieved from GBIF for the years 2000 to 2019, and environmental data from Bio-ORACLE v3.0 (Assis et al., 2024). We will use GLOSSA to fit a species distribution model and project it under different future climate scenarios.\nTo get started, we load the glossa package, as well as terra (Hijmans, 2024) and sf (Pebesma, 2018) to work with spatial rasters and vector data. We also load rgbif (Chamberlain, 2017) and biooracler (Fernandez Bejarano and Salazar, 2024) for downloading species occurrences and environmental data, respectively. Additionally, the dplyr (Wickham et al., 2023) package will be used for data manipulation.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(rgbif)\nlibrary(biooracler)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#data-preparation",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#data-preparation",
    "title": "Example 2",
    "section": "Data preparation",
    "text": "Data preparation\n\nDownload occurrence data\nWe will download occurrence data for Caretta caretta in the Mediterranean Sea using the GBIF API. The data will be filtered by date and geographic boundaries to fit the Mediterranean region and align with the temporal scale of the environmental data from Bio-ORACLE. To use the GBIF API, you need to have a registered account (user, pwd, and email). First, we retrieve the taxon key for C. caretta, and then use occ_download() from the rgbif package to download the occurrences. To restrict the download to the Mediterranean Sea, we use the pred_within() argument and define a polygon of interest (\"POLYGON ((-10 28, 38 28, 38 51, -10 51, -10 28))\"). Since Bio-ORACLE provides environmental layers in decadal steps, we limit the occurrence data to points between 2000 and 2019.\n\n# Function to retrieve taxon key from scientific name\nget_taxon_key &lt;- function(name) {\n  result &lt;- rgbif::occ_search(scientificName = name, hasCoordinate = TRUE)\n  if (!is.null(result$data)) {\n    return(as.character(result$data$taxonKey[1]))\n  } else {\n    warning(paste(\"No taxon key found for\", name))\n    return(NA)\n  }\n}\n\n# Define GBIF credentials and species information\nuser &lt;- \"&lt;gbif username&gt;\"\npwd &lt;- \"&lt;password&gt;\"\nemail &lt;- \"&lt;gbif mail&gt;\"\ntaxon_key &lt;- get_taxon_key(\"Caretta caretta\")\n\n# Download GBIF occurrence data\nrequest_id &lt;- as.character(rgbif::occ_download(\n  rgbif::pred(\"taxonKey\", taxon_key),\n  rgbif::pred(\"hasCoordinate\", TRUE),\n  rgbif::pred_within(\"POLYGON ((-10 28, 38 28, 38 51, -10 51, -10 28))\"),\n  rgbif::pred(\"occurrenceStatus\", \"PRESENT\"),\n  rgbif::pred_gte(\"YEAR\", 2000),\n  rgbif::pred_lte(\"YEAR\", 2019),\n  format = \"SIMPLE_CSV\",\n  user = user,\n  pwd = pwd,\n  email = email\n))\n\nresponse &lt;- rgbif::occ_download_wait(request_id)\n\nif (response$status == \"SUCCEEDED\"){\n  temp &lt;- tempfile(fileext = \".csv\")\n  download.file(response$downloadLink, temp, mode = \"wb\")\n  caretta &lt;- read.csv(unz(temp, paste0(response$key, \".csv\")),\n                        header = TRUE, sep = \"\\t\", dec = \".\")\n}\n\nOnce we have the dataset, we prepare it to fit the GLOSSA format. For this, we extract the required record locations (decimalLongitude and decimalLatitude) and assign a timestamp for each record. As Bio-ORACLE provides environmental layers for two decades (2000s and 2010s), we assign points from 2000 to 2009 to the first time period and points from 2010 to 2019 to the second time period. We then create a presence/absence column (pa), setting all downloaded records as presences. Finally, we save the file as a tab-separated CSV and retrieve the DOI of the downloaded data from GBIF (GBIF.org, 2024).\n\n# Prepare the data for modeling\n# Separate timestamp in two decades to fit Bio-ORACLE format (2000s and 2010s)\ncaretta &lt;- data.frame(\n  decimalLongitude = caretta$decimalLongitude,\n  decimalLatitude = caretta$decimalLatitude,\n  timestamp = ifelse(caretta$year %in% 2000:2009, 1, 2),\n  pa = 1\n)\ncaretta &lt;- caretta[complete.cases(caretta),]\n\n# Save the data\ndir.create(\"data\")\nwrite.table(caretta, file = \"data/Caretta_caretta_occ.csv\", sep = \"\\t\",\n            dec = \".\", quote = FALSE, row.names = FALSE)\n\n# Get citation for the downloaded data\ncitation &lt;- rgbif::gbif_citation(request_id)$download\ncitation\n# GBIF Occurrence Download https://doi.org/10.15468/dl.es7562\n# Accessed from R via rgbif (https://github.com/ropensci/rgbif) on 2024-08-27\n\n\n\nDownload environmental data\nBio-ORACLE provides environmental layers at a spatial resolution of 0.05 degrees and in decadal steps. We will download environmental variables such as ocean surface temperature (thetao in \\(^{\\circ}\\)C), primary productivity (phyc in \\(\\text{mmol} \\cdot \\text{m}^{-3}\\)), and salinity (so). Bathymetry (bat) will be obtained from the ETOPO 2022 Global Relief Model by NOAA (https://www.ncei.noaa.gov/products/etopo-global-relief-model).\n\n# Define temporal and spatial constraints\ntime = c(\"2000-01-01T00:00:00Z\", \"2010-01-01T00:00:00Z\")\nlatitude = c(28, 51)\nlongitude = c(-10, 38)\nconstraints = list(time, longitude, latitude)\nnames(constraints) = c(\"time\", \"longitude\", \"latitude\")\n\n# Download environmental layers from Bio-Oracle\nthetao_hist &lt;- download_layers(\"thetao_baseline_2000_2019_depthsurf\", \n                               \"thetao_mean\", constraints)\nphyc_hist &lt;- download_layers(\"phyc_baseline_2000_2020_depthsurf\", \n                             \"phyc_mean\", constraints)\nso_hist &lt;- download_layers(\"so_baseline_2000_2019_depthsurf\", \n                           \"so_mean\", constraints)\n\n# Download bathymetry data\ndownload.file(\"https://www.ngdc.noaa.gov/thredds/fileServer/global/ETOPO2022/60s/60s_bed_elev_netcdf/ETOPO_2022_v1_60s_N90W180_bed.nc\",\n              destfile = \"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\n\nWe will prepare the downloaded environmental data and save it in the directory structure required by GLOSSA.\n\ndir.create(\"data/fit_layers\")\n\n# Prepare environmental variables for modeling\nenv_var &lt;- list(thetao_hist, so_hist, phyc_hist)\nnames(env_var) &lt;- c(\"thetao\", \"so\", \"phyc\")\n\nfor (i in seq_along(env_var)) {\n  var_dir &lt;- paste0(\"data/fit_layers/\", names(env_var)[i])\n  dir.create(var_dir)\n  \n  terra::writeRaster(\n    env_var[[i]][[1]], \n    filename = paste0(var_dir, \"/\", names(env_var)[i], \"_1.tif\")\n  )\n  terra::writeRaster(\n    env_var[[i]][[2]],\n    filename = paste0(var_dir, \"/\", names(env_var)[i], \"_2.tif\")\n  )\n  \n  # Clean up auxiliary files generated by terra - Optional\n  aux_files &lt;- list.files(var_dir, pattern = \"\\\\.aux\\\\.json$\", full.names = TRUE)\n  file.remove(aux_files)\n}\n\n# Prepare bathymetry data\nbat &lt;- terra::rast(\"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\nbat &lt;- -1*bat # Change sign from elvation to bathymetry\n\n# Aggregate to match resolution with Bio-ORACLE layers\nbat &lt;- terra::aggregate(bat, fact = 3, fun = \"mean\")\nr &lt;- terra::rast(terra::ext(env_var[[1]]), res = terra::res(env_var[[1]]))\nbat &lt;- terra::resample(bat, r)\n\ndir.create(\"data/fit_layers/bat\")\nfor (i in 1:2){\n  terra::writeRaster(bat, filename = paste0(\"data/fit_layers/bat/bat_\", i, \".tif\"))\n}\n\n# Zip all prepared layers\nzip(zipfile = \"data/fit_layers.zip\", files = \"data/fit_layers\")\n\n\n\nClimate projections: SSP1 2.6 and SSP5 8.5\nWe will now download climate projections for SSP1 2.6 (sustainable development scenario where global CO2 emissions are strongly reduced but less rapidly) and SSP5 8.5 (a high emissions scenario) scenarios, and prepare the data for model projection.\n\nSSP1 2.6\n\n# SSP1 2.6 projections constraints\ntime = c(\"2020-01-01T00:00:00Z\", \"2090-01-01T00:00:00Z\")\nlatitude = c(28, 51)\nlongitude = c(-10, 38)\nconstraints = list(time, longitude, latitude)\nnames(constraints) = c(\"time\", \"longitude\", \"latitude\")\n\nenv_var_ssp126 &lt;- list(\n  thetao_ssp126 = download_layers(\"thetao_ssp126_2020_2100_depthsurf\", \n                                  \"thetao_mean\", constraints),\n  so_ssp126 = download_layers(\"so_ssp126_2020_2100_depthsurf\", \n                              \"so_mean\", constraints),\n  phyc_ssp126 = download_layers(\"phyc_ssp126_2020_2100_depthsurf\", \n                                \"phyc_mean\", constraints)\n)\nnames(env_var_ssp126) &lt;- c(\"thetao\", \"so\", \"phyc\")\n\ndir.create(\"data/proj_ssp126\")\nfor (i in seq_along(env_var_ssp126)) {\n  var_dir &lt;- paste0(\"data/proj_ssp126/\", names(env_var_ssp126)[i])\n  dir.create(var_dir, showWarnings = FALSE)\n  \n  terra::writeRaster(\n    env_var_ssp126[[i]][[1]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp126)[i], \"_1.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp126[[i]][[4]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp126)[i], \"_2.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp126[[i]][[8]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp126)[i], \"_3.tif\")\n  )\n  \n  # Clean up auxiliary files\n  aux_files &lt;- list.files(var_dir, pattern = \"\\\\.aux\\\\.json$\", full.names = TRUE)\n  file.remove(aux_files)\n}\n\n\n# Prepare bathymetry data for SSP1 2.6\ndir.create(\"data/proj_ssp126/bat\", showWarnings = FALSE)\nfor (i in 1:3) {\n  terra::writeRaster(bat, filename = paste0(\"data/proj_ssp126/bat/bat_\", i, \".tif\"))\n}\n\n# Zip the data\nzip(zipfile = \"data/proj_ssp126.zip\", files = \"data/proj_ssp126\")\n\n\n\nSSP5 8.5\n\n# SSP5 8.5 projections constraints\ntime = c(\"2020-01-01T00:00:00Z\", \"2090-01-01T00:00:00Z\")\nlatitude = c(28, 51)\nlongitude = c(-10, 38)\nconstraints = list(time, longitude, latitude)\nnames(constraints) = c(\"time\", \"longitude\", \"latitude\")\n\nenv_var_ssp585 &lt;- list(\n  thetao_ssp585 = download_layers(\"thetao_ssp585_2020_2100_depthsurf\", \n                                  \"thetao_mean\", constraints),\n  so_ssp585 = download_layers(\"so_ssp585_2020_2100_depthsurf\", \n                              \"so_mean\", constraints),\n  phyc_ssp585 = download_layers(\"phyc_ssp585_2020_2100_depthsurf\", \n                                \"phyc_mean\", constraints)\n)\nnames(env_var_ssp585) &lt;- c(\"thetao\", \"so\", \"phyc\")\n\n\ndir.create(\"data/proj_ssp585\")\nfor (i in seq_along(env_var_ssp585)) {\n  var_dir &lt;- paste0(\"data/proj_ssp585/\", names(env_var_ssp585)[i])\n  dir.create(var_dir, showWarnings = FALSE)\n  \n  terra::writeRaster(\n    env_var_ssp585[[i]][[1]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp585)[i], \"_1.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp585[[i]][[4]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp585)[i], \"_2.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp585[[i]][[8]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp585)[i], \"_3.tif\")\n  )\n  \n  # Clean up auxiliary files\n  aux_files &lt;- list.files(var_dir, pattern = \"\\\\.aux\\\\.json$\", full.names = TRUE)\n  file.remove(aux_files)\n}\n\n# Prepare bathymetry data for SSP5 8.5\ndir.create(\"data/proj_ssp585/bat\", showWarnings = FALSE)\nfor (i in 1:3) {\n  terra::writeRaster(bat, filename = paste0(\"data/proj_ssp585/bat/bat_\", i, \".tif\"))\n}\n\n# Zip the data\nzip(zipfile = \"data/proj_ssp585.zip\", files = \"data/proj_ssp585\")\n\n\n\n\nStudy area polygon\nTo define the study area, we downloaded a polygon representing the Mediterranean Sea, which restricts the analysis to this region as the environmental layers extend beyond it. The shapefile was obtained from the Marine Regions repository. The specific download link for the Mediterranean Sea polygon can be found here.\n\n\n\nPolygon defining the study area for the Mediterranean Sea. This polygon restricts the analysis to the Mediterranean region, considering the extent of the environmental layers. Image obtained from the Marine Regions website.\n\n\n\ntmpdir &lt;- tempdir()\nzip_contents &lt;- utils::unzip(\"data/iho.zip\", unzip = getOption(\"unzip\"), exdir = tmpdir)\nmed_sea &lt;- list.files(tmpdir, pattern = \"\\\\.shp$\", full.names = TRUE) %&gt;% \n  sf::st_read() %&gt;% \n  sf::st_geometry() %&gt;% \n  sf::st_union() %&gt;%\n  sf::st_make_valid()\nmed_sea &lt;- sf::st_geometry(med_sea[[2]]) %&gt;% \n  sf::st_make_valid()\nsf::st_crs(med_sea) &lt;- \"epsg:4326\"\nsf::st_write(med_sea, \"data/mediterranean_sea.gpkg\")"
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#glossa-modeling",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#glossa-modeling",
    "title": "Example 2",
    "section": "GLOSSA modeling",
    "text": "GLOSSA modeling\nWith the data prepared and formatted for the GLOSSA framework, we use the glossa::run_glossa() function to launch the GLOSSA Shiny app for species distribution modeling and projection under different climate scenarios.\n\nrun_glossa()\n\nUpload the occurrence file for C. caretta and the environmental layers for model fitting and projection scenarios. For habitat suitability analysis, select the Model fitting and Model projection options from the Suitable habitat model. Enable Variable importance in the Others section to check the decrease of the F-score.\nIn the advanced options, set the following parameters: thinning precision to 2, standardize environmental data, and enlarge the polygon by 0.01 degrees to account for the lower resolution of the polygon and ensure the boundaries match properly -previously, many points near the coast were lost due to an insufficient buffer size-. Select the BART (Chipman, et al., 2010; Dorie, 2024) model and set the seed to 5648 for reproducibility.\n\n\n\nAnalysis options set in GLOSSA for modeling the distribution of Caretta caretta."
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#results",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#results",
    "title": "Example 2",
    "section": "Results",
    "text": "Results\nOnce the analysis is completed, we observe occurrence records that were excluded due to being outside the study area or too close to other points (orange points in Figure 3), as we applied a thinning precision of 2. We are left with 5572 presence points and an equal number of pseudo-absences.\n\n\n\nValidation of presence points for Caretta caretta. Points are filtered based on their proximity and location relative to the study area, resulting in 5572 presence records (blue points).\n\n\nThe model summary indicates a decent predictive capacity with an AUC of 0.9 and an F-score of 0.8 The distribution of fitted values can also be explored. Note that the model was fitted using randomly generated pseudo-absences rather than real absences.\n\n\n\nSummary of the fitted model for Caretta caretta. From left to right: the ROC curve, a plot representing the confusion matrix with the associated probabilities, and the distribution on the fitted values.\n\n\nWe can also determine which predictors play a more significant role in predicting the outcome by examining the variable importance plot. This plot shows the decrease in model F-score using permutation feature importance, which measures the increase in prediction error after shuffling the predictor values. We see that the most important variable is the sea surface temperature followed by the bathymetry. Sea turtles live their entire lives in the ocean, but they migrate to nest on coastal land, with their sex being determined by the temperature during incubation (Mancino et al., 2022).\n\n\n\nVariable importance plot using the F-score as the performance metric\n\n\nFor the climate projections, there is a predicted decrease of 16.4% in suitable habitat area (\\(\\text{km}^2\\)) in the SSP1-2.6 scenario and about 78.2% in the SSP5-8.5 scenario from the 2020s to the 2090s. The western Mediterranean and the Adriatic Sea are projected to have relatively better habitat suitability.\n\n\n\nProjected changes in suitable habitat for Caretta caretta under different climate scenarios."
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#conclusion",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#conclusion",
    "title": "Example 2",
    "section": "Conclusion",
    "text": "Conclusion\nThis example shows how GLOSSA can be used to model the distribution of Caretta caretta in the Mediterranean Sea. The steps included downloading occurrence data from GBIF, obtaining environmental layers from Bio-ORACLE, and preparing the data for model fitting and projections under various climate change scenarios."
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#computation-time",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#computation-time",
    "title": "Example 2",
    "section": "Computation time",
    "text": "Computation time\nThe analysis was performed on a single Windows 11 machine equipped with 64 GB of RAM and an Intel(R) Core(TM) i7-1165G7 processor. This processor features 4 cores and 8 threads, with a base clock speed of 2.80 GHz. The following table summarizes the computation times for various stages of the GLOSSA analysis. This provides an overview of the computational resources required for each step in the analysis.\n\nTable 1. Computation times for different stages of the GLOSSA analysis for the loggerhead sea turtle (Caretta caretta) in the Mediterranean Sea.\n\n\n\n\nTask\nExecution Time\n\n\n\n\nLoading input data\n33.65 secs\n\n\nProcessing P/A coordinates\n0.21 secs\n\n\nProcessing covariate layers\n9.80 secs\n\n\nBuilding model matrix\n12.01 secs\n\n\nFitting native range models\n0.232 mins\n\n\nVariable importance (native range)\n8.31 mins\n\n\nP/A cutoff (native range)\n0.191 mins\n\n\nProjections on fit layers (native range)\n9.84 mins\n\n\nNative range projections\n9.47 mins\n\n\nNative range\n19.74 mins\n\n\nFitting suitable habitat models\n0.232 mins\n\n\nVariable importance (suitable habitat)\n8.31 mins\n\n\nP/A cutoff (suitable habitat)\n0.191 mins\n\n\nProjections on fit layers (suitable habitat)\n9.84 mins\n\n\nSuitable habitat projections\n9.47 mins\n\n\nHabitat suitability\n0.006 mins\n\n\nSuitable habitat\n19.74 mins\n\n\nComputing functional responses\n13.35 mins\n\n\nCross-validation\n4.19 mins\n\n\nModel summary\n0.18 mins\n\n\nTotal GLOSSA analysis\n38.39 mins"
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#references",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#references",
    "title": "Example 2",
    "section": "References",
    "text": "References\n\nAssis, J., Fernández Bejarano, S. J., Salazar, V. W., Schepers, L., Gouvea, L., Fragkopoulou, E., … & De Clerck, O. (2024). Bio‐ORACLE v3. 0. Pushing marine data layers to the CMIP6 Earth System Models of climate change research. Global Ecology and Biogeography, 33(4), e13813.\nChamberlain, S. (2017). rgbif: Interface to the Global Biodiversity Information Facility API. R package version 0.9.8. https://CRAN.R-project.org/package=rgbif\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees.\nDorie V (2024). dbarts: Discrete Bayesian Additive Regression Trees Sampler. R package version 0.9-28, https://CRAN.R-project.org/package=dbarts.\nFernandez-Bejarano, S. J. & Salazar, V. W. (2024). biooraclee: R package to access Bio-Oracle data via ERDDAP. R package version 0.0.0.9, https://github.com/bio-oracle/biooracler\nGBIF.org (26 August 2024) GBIF Occurrence Download https://doi.org/10.15468/dl.es7562\nHijmans, R. (2024). terra: Spatial Data Analysis. R package version 1.7-81, https://rspatial.github.io/terra/, https://rspatial.org/.\nMancino, C., Canestrelli, D., & Maiorano, L. (2022). Going west: Range expansion for loggerhead sea turtles in the Mediterranean Sea under climate change. Global Ecology and Conservation, 38, e02264.\nPebesma E (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10(1), 439–446. doi:10.32614/RJ-2018-009.\nWickham H, François R, Henry L, Müller K & Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.4, https://github.com/tidyverse/dplyr, https://dplyr.tidyverse.org."
  },
  {
    "objectID": "index2.html",
    "href": "index2.html",
    "title": "GLOSSA Documentation - Coming Soon!",
    "section": "",
    "text": "Our team is hard at work to provide you with comprehensive guides and resources.\nThank you for your patience. Stay tuned for updates!"
  },
  {
    "objectID": "how_to_cite.html",
    "href": "how_to_cite.html",
    "title": "How to cite GLOSSA",
    "section": "",
    "text": "How to cite GLOSSA\nThank you for using GLOSSA! Tu cite our app, please use the following references.\n\nGLOSSA GitHub repository\nThe GLOSSA paper is currently in progress. Meanwhile, you can reference the GitHub repository as follows:\n\nMestre-Tomás, J., Fuster-Alonso, A., & Coll, M. (2024) GLOSSA: A user-friendly Shiny app for global spatiotemporal analysis of species with BART machine learning model. R package version 0.1.0, https://github.com/jmestret/glossa\n\n\n\nBART model reference\nIf you use the BART model in your work, please also cite the original paper:\n\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian Additive Regression Trees. The Annals of Applied Statistics, 4(1), 266–298. doi: https://doi.org/10.1214/09-AOAS285\n\nand if use BART on a global-scale, please include this citation:\n\nFuster-Alonso, A., Mestre-Tomás, J., Baez, J. C., Pennino, M. G., Barber, X., Bellido, J. M., … & Coll, M. (2024). Machine learning applied to global scale species distribution models (SDMs). PREPRINT (Version 1) available at Research Square^. doi: https://doi.org/10.21203/rs.3.rs-4411399/v1."
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "We are working to provide teaching materials!",
    "section": "",
    "text": "Thank you for your patience. Stay tuned for updates!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "contact_us.html",
    "href": "contact_us.html",
    "title": "Contact us",
    "section": "",
    "text": "We value your feedback and are here to help you with any questions, support needs or collaboration requests. Please use the following means to get in touch with us:\n\n\nFor issues, feature requests, questions, or general feedback, please use the Issues tab on our GitHub repository:\n\nGLOSSA GitHub\n\n\n\n\nFor special support or collaboration requests, please contact us via the contact form at the end of the iMARES website:\n\niMARES\n\n\n\n\nFollow our research group on Twitter for the latest news and updates:\n\n@iMARES_group\n\nThank you for your support! If you want to know more about the group go and visit our website: iMARES"
  },
  {
    "objectID": "contact_us.html#github",
    "href": "contact_us.html#github",
    "title": "Contact us",
    "section": "",
    "text": "For issues, feature requests, questions, or general feedback, please use the Issues tab on our GitHub repository:\n\nGLOSSA GitHub"
  },
  {
    "objectID": "contact_us.html#contact-form",
    "href": "contact_us.html#contact-form",
    "title": "Contact us",
    "section": "",
    "text": "For special support or collaboration requests, please contact us via the contact form at the end of the iMARES website:\n\niMARES"
  },
  {
    "objectID": "contact_us.html#x-twitter",
    "href": "contact_us.html#x-twitter",
    "title": "Contact us",
    "section": "",
    "text": "Follow our research group on Twitter for the latest news and updates:\n\n@iMARES_group\n\nThank you for your support! If you want to know more about the group go and visit our website: iMARES"
  },
  {
    "objectID": "get_started.html",
    "href": "get_started.html",
    "title": "Get started",
    "section": "",
    "text": "Welcome to GLOSSA, our Shiny app for species distribution modeling! This tool allows you to predict species distributions using occurrence data and environmental data. Below is a quick guide to help you get started. Please ensure that you have R version 4.0.0 or higher installed before proceeding:"
  },
  {
    "objectID": "get_started.html#installation",
    "href": "get_started.html#installation",
    "title": "Get started",
    "section": "Installation",
    "text": "Installation\nTo install the GLOSSA shiny app, run the following command in your R console:\ninstall.packages(\"glossa\")\nAlternatively, you can install the latest development version from GitHub to get the newest updates and fixes:\nif (!require(\"devtools\")) \n  install.packages(\"devtools\")\n\ndevtools::install_github(\"jmestret/glossa\")"
  },
  {
    "objectID": "get_started.html#glossa-workflow-summary",
    "href": "get_started.html#glossa-workflow-summary",
    "title": "Get started",
    "section": "GLOSSA workflow summary",
    "text": "GLOSSA workflow summary\n\nData input:\n\nLoad species occurrence data: Upload species occurrence data, either presence-absence or presence-only (with pseudo-absences generated). Multiple species can be modeled in one session.\nUpload environmental data: Provide environmental variables in raster format, used as predictors in the model. These layers also define the study area unless a polygon is provided.\nUpload projection layers: Optionally, upload layers to forecast for different time periods or climate scenarios.\nDefine study area: Optionally, upload a polygon to define the study area, which will filter occurrences and crop environmental layers.\n\nData processing:\n\nCoordinate cleaning: GLOSSA cleans coordinates by removing duplicates and points outside the study area.\nLayers processing: Environmental layers are cropped, masked, and optionally standardized using Z-score.\nGenerate pseudo-absences: For presence-only data, pseudo-absences are randomly generated.\n\nModel fitting and Pprediction:\n\nFit BART model: Two models are fitted—one based on environmental data and another including spatial smoothing.\nModel output: Predict species occurrence probabilities and determine optimal classification cutoffs. Evaluate predictive performance using cross-validation, and assess variable importance and functional responses.\nProjections: Generate projections for different areas, time periods, and climate scenarios using the Bayesian framework.\n\nVisualization and export:\n\nExplore and save interactive results once the analysis is complete."
  },
  {
    "objectID": "get_started.html#quick-start-guide",
    "href": "get_started.html#quick-start-guide",
    "title": "Get started",
    "section": "Quick start guide",
    "text": "Quick start guide\nHere’s a quick guide on how to use the app. First, launch the app by running the run_glossa() function:\nlibrary(glossa)\nrun_glossa()\nThis will open the app on the Home tab, the landing page of the app. From here, you can start a new analysis, watch our demo video, explore the documentation, read tutorials, or even meet the team that built the app!\n\n\n\n\n\nClicking on the question mark icon  on the top right corner will bring up a brief explanation of what each tab and button does.\n\n\n\n\n\n\nSidebar overview\nOn the sidebar, you’ll find two main sections: Modelling and Resources.\n\nResources: This section provides essential information to help you run the app. It’s a great place to refresh your knowledge, especially if it’s been a while since you last used GLOSSA. Note that the full documentation is hosted outside the app in this website.\nModelling: This section includes the New Analysis, Reports, and Exports tabs. Typically, you’ll go through these tabs in sequence:\n\nNew Analysis: Set up your analysis, upload your data, and select analysis options.\nReports: Explore the results and visualizations generated by the analysis.\nExports: Download the results for further use."
  },
  {
    "objectID": "get_started.html#running-your-first-analysis",
    "href": "get_started.html#running-your-first-analysis",
    "title": "Get started",
    "section": "Running your first analysis",
    "text": "Running your first analysis\nTo run your first analysis, go to the New Analysis tab. This tab looks like this:\n\n\n\n\n\nHere, you need to upload the required input files and select your analysis options:\n\nData upload: The first panel is where you upload your data and configure the analysis settings.\nPrevisualization: The second panel provides an interactive map to preview your input data.\nPredictor variables: Choose predictor variables for each uploaded species.\nUploaded files: A table indicates if your input files are formatted correctly.\n\n\nRequired files for analysis\nGLOSSA can function with just occurrence data and environmental variables, but additional options are available. Let’s briefly go through the necessary files:\n\nOccurrences: Upload a tab-separated CSV file with four columns indicating the occurrence location, whether it is a presence or absence, and the time it was recorded. The columns must be named exactly as follows: decimalLongitude, decimalLatitude, pa, and timestamp. If the pa column is missing, GLOSSA will assume all rows are presences. If timestamp is missing, GLOSSA will assume all observations occurred at the same time. GLOSSA also supports presence-only data but will generate randomly distributed balanced pseudo-absences to fit the model.\n&gt; head(sp1)\n  decimalLongitude decimalLatitude timestamp pa\n1          5.42909        43.20937         1  1\n2           -43.05           49.03         1  0\n3         -2.52369        47.29234         2  1\n4           34.054         -26.913         2  1\n5           -41.63            46.3         2  0\n6           -174.5            27.5         3  1\nEnvironmental data: Upload environmental data as raster files (e.g., .tif or .nc format) in a ZIP file with a specific structure. The ZIP file should contain a subdirectory for each environmental variable, with files sorted by time period. For example, if you have two variables (x1 and x2) and your observations are from two different years, your ZIP file should look like this:\nfit_layers.zip\n    ├───x1\n    │       x1_1.tif\n    │       x1_2.tif\n    └───x2\n            x2_1.tif\n            x2_2.tif\nEnsure that all layers have the same resolution, the same number of layers, and that they match the number of years in your occurrence files. If you want to use the same layer for all observations, include just one file in the subdirectory and set all timestamp values to 1 or remove the timestamp column.\nProjection layers (Optional): If you want to make predictions, upload your projection data here. This file has the same format as the environmental data file, and the subdirectories must match those used for fitting the model. You can upload multiple ZIP files if you want to predict multiple scenarios (e.g., different temperature increase scenarios).\nStudy area (Optional): If your rasters cover a larger area than your study region, you can provide a polygon to delimit your study area (formats: GPKG, KML or GeoJSON). This will remove points outside the polygon and mask the environmental variables accordingly.\n\nOnce all files are uploaded, if they pass the checks and are properly formatted, the table in the bottom right panel will show a checkmark for each file. If something is incorrect, refer to the documentation for a quick solution.\n\n\nAnalysis options\nIn this section, you need to select the desired output. GLOSSA fits two kinds of models:\n\nNative range: The model includes environmental variables and uses longitude and latitude coordinates as a spatial smoother.\nSuitable habitat: The model only includes the environmental variables.\n\nYou can choose to fit the model under the Model fitting option. This option will fit the model, compute variable importance, and generate a prediction map representing the mean environmental conditions across all provided layers. If you’ve uploaded projection layers and want to make predictions using the fitted model, select the Model projection option.\nWhen fitting the model, if multiple years or time periods are uploaded, GLOSSA will extract the value of the corresponding environmental layer for each occurrence based on the specific time stamp.\nAdditionally, you can check the Functional responses checkbox if you want to compute the response curves (i.e., the relationship between the occurrence of a species and each environmental variable). You can also enable Cross-validation, which will perform a K-fold cross-validation with (k = 5).\n\n\nAdvanced options\nBy selecting the Advanced options button, a sidebar will appear with extra options for refining your analysis:\n\nOccurrences thinning: Specify the number of decimal places to round coordinates, allowing you to apply spatial thinning to your occurrence data using a precision-based method. GLOSSA currently implements this method via the GeoThinneR R package, which is the most time- and memory-efficient option for large datasets. If you need to perform spatial thinning based on distance or a grid, you can do so before uploading your data to GLOSSA using GeoThinneR or other methods.\nStandardize covariates: GLOSSA uses a scaling method that subtracts the mean and divides by the standard deviation for standardization. The mean and standard deviation are calculated from the fitting layers, and the same values are used to standardize the projection layers, ensuring consistency across variables.\nEnlarge polygon: If your polygon has low resolution, you can apply a buffer in degrees to expand it. This is useful if you have points near the coast that fall outside the polygon due to poor resolution. You can preview the buffer using the “play” icon before running the analysis to find the optimal value.\nModel: Choose the model to apply. Currently, GLOSSA only supports the BART model, so no additional selection is needed here.\nSet a seed: Specify a seed for reproducibility of your results.\n\n\n\n\n\n\n\n\nPredictor variables\nIf you uploaded multiple species, you can select different predictor variables for each species in the bottom left panel.\n\n\n\n\n\n\n\nUploaded files\nIn the table in the bottom right corner, ensure that all files are checked and that you’ve selected your analysis options.\n\n\n\n\n\nOnce everything is set, you’re ready to run the analysis. Click the Run Job button, confirm in the dialog, and wait for the analysis to complete."
  },
  {
    "objectID": "get_started.html#analysis-results",
    "href": "get_started.html#analysis-results",
    "title": "Get started",
    "section": "Analysis results",
    "text": "Analysis results\nOnce the analysis is complete, you will be redirected to the Reports tab, where you can explore all the results and export visualizations .\n\n\n\n\n\nIn the top left corner of the tab, you can select the species for which you want to view the results. The first row displays key metrics:\n\nPotential suitable area: Calculated in square kilometers, based on the predicted presence-absence grid cells.\nMean suitable probability: The average probability of suitable habitat across the entire prediction area.\nPresences/Absences: The number of presence and absence points used to fit the model after all processing and cleaning.\n\nIf multiple projection layers are provided (for example, a time series), a sparkline plot will display the values for each time period, and the text value shown will represent the last one in the time series.\n\nGLOSSA predictions\nThe first plot, titled GLOSSA predictions, shows the presence probability predictions within the study area. As we are working in the Bayesian framework, each grid cell has associated one predictive posterior distribution, therefore we can obtain a more comprenhensive undertanding of the predictions by exploring metrics like the mean, median or quantiles. Using the three-dot icon , you can open the sidebar to customize the display:\n\nChoose between predictions on the fitting layers or the projection layers.\nToggle between viewing the native range or the suitable habitat.\nSelect which value from the posterior distribution to display (e.g., mean probability, median, etc.).\n\n\n\n\n\n\n\n\nEnvironmental variables and Presence validation\nThe plot on the right shows the environmental variables used to fit the model, allowing you to quickly compare them with the probability projections. Below this, another plot displays the occurrence points that were retained or filtered out during the analysis.\n\n\n\n\n\n\n\nFunctional responses and Variable importance\nIn the last row, you can view:\n\nFunctional response: These illustrate the relationship between each environmental variable and the predicted suitable habitat (response curves).\nVariable importance: Displays the importance of each variable for both the native range and the suitable habitat.\n\n\n\n\n\n\n\n\nCross-validation\nIn the cross-validation panel, you’ll find performance metrics such as:\n\nAIC (Akaike Information Criterion)\nRMSE (Root Mean Square Error)\n5-Fold Cross-Validation Results"
  },
  {
    "objectID": "get_started.html#exports",
    "href": "get_started.html#exports",
    "title": "Get started",
    "section": "Exports",
    "text": "Exports\nIn the Exports tab, you can export the results of your analysis. This includes all projection maps, the data used to fit the model, variable importance metrics, cross-validation results, etc. You can export almost everything, enabling you to explore the results further or create your own visualizations for your work!"
  },
  {
    "objectID": "get_started.html#next-steps",
    "href": "get_started.html#next-steps",
    "title": "Get started",
    "section": "Next Steps",
    "text": "Next Steps\nThis Quick start guide for GLOSSA should help you get up and running. For more detailed instructions, check out the Tutorial tab, where you’ll find tutorials on how to prepare your data for GLOSSA or even worked examples that guide you through using GLOSSA from end-to-end.\nAdditionally, the Documentation tab offers a comprehensive guide to every aspect of the GLOSSA app. Here, you’ll find not only in-depth information on how GLOSSA works but also tips and tricks to help you get the most out of the app.\nIf you have further questions, visit the FAQs tab, or if you still need help, you can reach us through the Contact Us tab.\nThank you for using GLOSSA, and enjoy your species distribution modeling!"
  },
  {
    "objectID": "pages/guide/index.html",
    "href": "pages/guide/index.html",
    "title": "GLOSSA Documentation - Coming Soon!",
    "section": "",
    "text": "Our team is hard at work to provide you with comprehensive guides and resources.\nThank you for your patience. Stay tuned for updates!"
  },
  {
    "objectID": "pages/tutorials_examples/index.html",
    "href": "pages/tutorials_examples/index.html",
    "title": "Tutorials and Examples",
    "section": "",
    "text": "Example 1\n\n\nWorldwide Suitable Habitat of Thunnus albacares\n\n\n6 min\n\n\nThis example demonstrates how to use the GLOSSA framework to model the suitable habitat of Thunnus albacares (yellowfin tuna) on a global scale under different climate…\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\nDistribution of the Loggerhead Sea Turtle in the Mediterranean Sea\n\n\n17 min\n\n\nThis vignette provides a detailed example of fitting a single-species distribution model within a user-defined region.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3\n\n\nPotential Risk Areas of Siganus luridus in the Greek Seas\n\n\n12 min\n\n\nThis example shows how to use GLOSSA to predict the suitable habitat of Siganus luridus in the Greek Seas and identify potential areas at risk of invasion.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html",
    "title": "Example 1",
    "section": "",
    "text": "This example demonstrates how to use the GLOSSA framework to model the suitable habitat of Thunnus albacares (yellowfin tuna) on a global scale under different climate scenarios. We will use occurrence data from 1850 to 2014, downloaded from OBIS (Accessed on 26/08/2024), and historical and future environmental projections from ISIMIP (https://data.isimip.org/). The goal is to predict how habitat suitability for Thunnus albacares may change under the SSP1-2.6 (sustainable development) and SSP5-8.5 (high emissions) climate scenarios.\nFirst, we will load the necessary R packages.\n\nlibrary(glossa)\nlibrary(robis)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#introduction",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#introduction",
    "title": "Example 1",
    "section": "",
    "text": "This example demonstrates how to use the GLOSSA framework to model the suitable habitat of Thunnus albacares (yellowfin tuna) on a global scale under different climate scenarios. We will use occurrence data from 1850 to 2014, downloaded from OBIS (Accessed on 26/08/2024), and historical and future environmental projections from ISIMIP (https://data.isimip.org/). The goal is to predict how habitat suitability for Thunnus albacares may change under the SSP1-2.6 (sustainable development) and SSP5-8.5 (high emissions) climate scenarios.\nFirst, we will load the necessary R packages.\n\nlibrary(glossa)\nlibrary(robis)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#data-preparation",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#data-preparation",
    "title": "Example 1",
    "section": "Data preparation",
    "text": "Data preparation\n\nDownload occurrence data\nWe will download occurrence data for Thunnus albacares from the OBIS database (https://obis.org/taxon/127027) using the robis package. After that, we’ll select the necessary columns for GLOSSA (decimalLongitude, decimalLatitude, timestamp, and pa), and since all records indicate presences, we’ll replace them with a value of 1 (where 1 represents presence and 0 absence, as required by GLOSSA). We will remove records with missing values and filter the data to include only records from 1850 to 2014, aligning with the temporal coverage of our environmental layers.\n\n# Download data from OBIS\nalbacares &lt;- robis::occurrence(scientificname = \"Thunnus albacares\")\n\n# Format data to fit GLOSSA\n# Select and rename columns of interest\nalbacares &lt;- albacares[, c(\"decimalLongitude\", \"decimalLatitude\", \"date_year\", \"occurrenceStatus\")]\ncolnames(albacares) &lt;- c(\"decimalLongitude\", \"decimalLatitude\", \"timestamp\", \"pa\")\n\n# Convert presence data to 1 (for presences)\ntable(albacares$pa)\nalbacares &lt;- albacares[albacares$pa %in% c(\"present\", \"Present\", \"Presente\", \"P\"), ] # In this case, we only have presences\nalbacares$pa &lt;- 1\n\n# Remove incomplete records (with NA values)\nalbacares &lt;- albacares[complete.cases(albacares), ]\n\n# Filter study period to match environmental variables (1850-2014)\nalbacares &lt;- albacares[albacares$timestamp &gt;= 1850 & albacares$timestamp &lt;= 2014, ]\n\n# Save to file\nwrite.table(as.data.frame(albacares), file = \"data/Thunnus_albacares_occ.csv\", sep = \"\\t\", dec = \".\", quote = FALSE)\n\n\n\nDownload environmental data\nNext, we will download environmental data from ISIMIP, using the GFDL-ESM4 Earth System Model from the ISIMIP3b climate dataset. The variables we’ll download include:\n\nSea surface temperature (tos)\nSea surface water salinity (so)\nPhytoplankton content vertically integrated over all oceans levels (phyc)\nSurface concentration of dissolved molecular oxygen (o2)\n\nAdditionally, we will download bathymetry data from the ETOPO 2022 Global Relief Model by NOAA to include in our analysis. We downloaded the bedrock elevation netCDF version ETOPO 2022 with a 60 arc-second resolution.\n\n# Load environmental data layers\nenv_data &lt;- list(\n  tos = terra::rast(\"data/gfdl-esm4_r1i1p1f1_historical_tos_60arcmin_global_monthly_1850_2014.nc\"),\n  so = terra::rast(\"data/gfdl-esm4_r1i1p1f1_historical_so-surf_60arcmin_global_monthly_1850_2014.nc\"),\n  phyc = terra::rast(\"data/gfdl-esm4_r1i1p1f1_historical_phyc-vint_60arcmin_global_monthly_1850_2014.nc\"),\n  o2 = terra::rast(\"data/gfdl-esm4_r1i1p1f1_historical_o2-surf_60arcmin_global_monthly_1850_2014.nc\")\n)\n\n# Process data to calculate annual means for each variable\nenv_data_year &lt;- list()\nn &lt;- 1\nfor (i in seq(from = 1056, to = (1980 - 12), by = 12)){\n  for (j in names(env_data)){\n    env_data_year[[j]][[n]] &lt;- terra::mean(env_data[[j]][[i:(i+11)]])\n  }\n  n &lt;- n + 1\n}\n\n# Prepare bathymetry data and resample to match environmental data resolution\nbat &lt;- terra::rast(\"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\nbat &lt;- -1*bat\nbat &lt;- terra::aggregate(bat, fact = 60, fun = \"mean\")\nr &lt;- terra::rast(terra::ext(env_data_year[[1]][[1]]), res = terra::res(env_data_year[[1]][[1]]))\nbat &lt;- terra::resample(bat, r)\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save processed layers to files\ndir.create(\"data/fit_layers\")\ndir.create(\"data/fit_layers/bat\")\ndir.create(\"data/fit_layers/tos\")\ndir.create(\"data/fit_layers/so\")\ndir.create(\"data/fit_layers/phyc\")\ndir.create(\"data/fit_layers/o2\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(env_data_year[[j]][[i]], filename = paste0(\"data/fit_layers/\", j ,\"/\", j, \"_\", i, \".tif\"))\n  }\n}\n\n# Zip the output files\nzip(zipfile = \"data/fit_layers.zip\", files = \"data/fit_layers\")\n\n\n\nClimate projections: SSP1-2.6 and SSP5-8.5\nWe will now download climate projections for the SSP1-2.6 and SSP5-8.5 scenarios, which represent two different future climate pathways. The SSP1-2.6 scenario assumes sustainable development and slower reductions in CO2 emissions, while the SSP5-8.5 scenario assumes a high-emissions future.\n\nSSP1-2.6\nThe following environmental variables will be downloaded for the SSP1-2.6 scenario:\n\ntos: Sea surface temperature\nso: Sea surface water salinity\nphyc: Phytoplankton content\no2: Surface molecular oxygen concentration\n\n\n# Load SSP1-2.6 climate projection data\nenv_data &lt;- list(\n  tos = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp126_tos_60arcmin_global_monthly_2015_2100.nc\"),\n  so = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp126_so-surf_60arcmin_global_monthly_2015_2100.nc\"),\n  phyc = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp126_phyc-vint_60arcmin_global_monthly_2015_2100.nc\"),\n  o2 = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp126_o2-surf_60arcmin_global_monthly_2015_2100.nc\")\n)\n\n# Extract data for 2025, 2050, and 2100\nenv_data_year &lt;- list()\nn &lt;- 1\nfor (i in c(121, 421, 1021)){\n  for (j in names(env_data)){\n    env_data_year[[j]][[n]] &lt;- terra::mean(env_data[[j]][[i:(i+11)]])\n  }\n  n &lt;- n + 1\n}\n\n# Add bathymetry data to the layers\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save projection data to files\ndir.create(\"data/proj_ssp126\")\ndir.create(\"data/proj_ssp126/bat\")\ndir.create(\"data/proj_ssp126/tos\")\ndir.create(\"data/proj_ssp126/so\")\ndir.create(\"data/proj_ssp126/phyc\")\ndir.create(\"data/proj_ssp126/o2\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(env_data_year[[j]][[i]], filename = paste0(\"data/proj_ssp126/\", j ,\"/\", j, \"_\", i, \".tif\"))\n  }\n}\n\n# Zip the output files\nzip(zipfile = \"data/proj_ssp126.zip\", files = \"data/proj_ssp126\")\n\n\n\nSSP5-8.5\nSimilarly, we will download environmental data for the SSP5-8.5 scenario.\n\ntos: Sea surface temperature\nso: Sea surface water salinity\nphyc: Phytoplankton content\no2: Surface molecular oxygen concentration\n\n\n# Load SSP5-8.5 climate projection data\nenv_data &lt;- list(\n  tos = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp585_tos_60arcmin_global_monthly_2015_2100.nc\"),\n  so = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp585_so-surf_60arcmin_global_monthly_2015_2100.nc\"),\n  phyc = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp585_phyc-vint_60arcmin_global_monthly_2015_2100.nc\"),\n  o2 = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp585_o2-surf_60arcmin_global_monthly_2015_2100.nc\")\n)\n\n# Extract data for 2025, 2050, and 2100\nenv_data_year &lt;- list()\nn &lt;- 1\nfor (i in c(121, 421, 1021)){\n  for (j in names(env_data)){\n    env_data_year[[j]][[n]] &lt;- terra::mean(env_data[[j]][[i:(i+11)]])\n  }\n  n &lt;- n + 1\n}\n\n# Add bathymetry data to the layers\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save projection data to files\ndir.create(\"data/proj_ssp585\")\ndir.create(\"data/proj_ssp585/bat\")\ndir.create(\"data/proj_ssp585/tos\")\ndir.create(\"data/proj_ssp585/so\")\ndir.create(\"data/proj_ssp585/phyc\")\ndir.create(\"data/proj_ssp585/o2\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(env_data_year[[j]][[i]], filename = paste0(\"data/proj_ssp585/\", j ,\"/\", j, \"_\", i, \".tif\"))\n  }\n}\n\n# Zip the output files\nzip(zipfile = \"data/proj_ssp585.zip\", files = \"data/proj_ssp585\")"
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#glossa-modeling",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#glossa-modeling",
    "title": "Example 1",
    "section": "GLOSSA modeling",
    "text": "GLOSSA modeling\nNow that we have our environmental data ready, we can proceed with fitting the habitat suitability model using GLOSSA.\n\nrun_glossa()\n\nWe upload the occurrence file for T. albacares along with the environmental layers for model fitting and the two projection scenarios. Since our goal is to compute suitable habitat, we select the Model fitting and Model projection options from the Suitable habitat model. Additionally, we enable Functional responses in the Others section to compute the response curves.\nIn the advanced options, we adjust the settings as follows: we select a thinning precision of 2 and standardize the environmental data. We choose the BART model (Chipman, et al., 2010; Dorie, 2024) and set the seed to 4572 for reproducibility.\nToDo: Add a section with the details of the GLOSSA model implementation and interpretation of the results."
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#references",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#references",
    "title": "Example 1",
    "section": "References",
    "text": "References\n\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees.\nDorie V (2024). dbarts: Discrete Bayesian Additive Regression Trees Sampler. R package version 0.9-28, https://CRAN.R-project.org/package=dbarts.\nISIMIP Project, 2024. Climate projections data. Available at https://data.isimip.org/.\nOBIS (2024) Ocean Biodiversity Information System. Intergovernmental Oceanographic Commission of UNESCO. https://obis.org.\nNOAA National Centers for Environmental Information, 2022. ETOPO 2022 Global Relief Model. Available at https://www.ncei.noaa.gov/products/etopo-global-relief-model."
  }
]